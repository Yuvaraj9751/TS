# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YzqzIKSGkkRGRAK-9pO5CoKww3F-bjBY
"""

import os import argparse import math import json from datetime import datetime, timedelta

import numpy as np import pandas as pd import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error

import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader

Optional: SARIMA

try: from statsmodels.tsa.statespace.sarimax import SARIMAX STATSMODELS_AVAILABLE = True except Exception: STATSMODELS_AVAILABLE = False

---------------------------

Utilities & Data Generation

---------------------------

def generate_synthetic_timeseries(start_date='2016-01-01', days=365*5, seed=42): np.random.seed(seed) dates = pd.date_range(start=start_date, periods=days, freq='D') t = np.arange(days) # Components: trend, yearly seasonality, weekly seasonality, random noise, occasional events trend = 0.0005 * t  # slow upward trend yearly = 10 * np.sin(2 * np.pi * t / 365.25) weekly = 2.0 * np.sin(2 * np.pi * t / 7) noise = np.random.normal(scale=1.5, size=days) # external regressor: temperature-like (seasonal) and holiday effect temp = 20 + 8 * np.sin(2 * np.pi * (t + 30) / 365.25) + np.random.normal(scale=1.0, size=days) holiday = np.zeros(days) # add some occasional jumps (external events) event_days = np.random.choice(days, size=8, replace=False) for d in event_days: holiday[d:d+3] += np.random.uniform(5, 15) series = 50 + trend + yearly + weekly + 0.3 * temp + holiday + noise df = pd.DataFrame({'ds': dates, 'y': series, 'temp': temp, 'holiday': holiday}) return df

lag features for baseline

def create_lag_features(df, lags=[1,7,14,30]): out = df.copy() for lag in lags: out[f'y_lag_{lag}'] = out['y'].shift(lag) out = out.dropna().reset_index(drop=True) return out

---------------------------

PyTorch Dataset

---------------------------

class TimeSeriesDataset(Dataset): def init(self, df, input_window=60, output_window=14, feature_cols=['y','temp','holiday']): self.df = df.reset_index(drop=True) self.input_window = input_window self.output_window = output_window self.feature_cols = feature_cols self.X, self.Y = self.create_windows()

def create_windows(self):
    arr = self.df[self.feature_cols].values.astype(np.float32)
    n = len(arr)
    X, Y = [], []
    for i in range(n - self.input_window - self.output_window + 1):
        X.append(arr[i:i+self.input_window])
        Y.append(arr[i+self.input_window:i+self.input_window+self.output_window, 0])
    return np.array(X), np.array(Y)

def __len__(self):
    return len(self.X)

def __getitem__(self, idx):
    return self.X[idx], self.Y[idx]

---------------------------

Seq2Seq with Bahdanau Attention (PyTorch)

---------------------------

class Encoder(nn.Module): def init(self, input_dim, hidden_dim, n_layers=1): super().init() self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)

def forward(self, x):
    outputs, (hidden, cell) = self.rnn(x)
    return outputs, (hidden, cell)

class BahdanauAttention(nn.Module): def init(self, enc_hidden_dim, dec_hidden_dim): super().init() self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim) self.v = nn.Linear(dec_hidden_dim, 1, bias=False)

def forward(self, hidden, encoder_outputs):
    # hidden: (batch, dec_hidden); encoder_outputs: (batch, seq_len, enc_hidden)
    seq_len = encoder_outputs.size(1)
    hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)
    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
    attention = torch.softmax(self.v(energy).squeeze(2), dim=1)
    return attention

class Decoder(nn.Module): def init(self, input_dim, enc_hidden_dim, dec_hidden_dim, output_dim=1, n_layers=1): super().init() self.rnn = nn.LSTM(input_dim + enc_hidden_dim, dec_hidden_dim, n_layers, batch_first=True) self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim) self.out = nn.Linear(dec_hidden_dim, output_dim)

def forward(self, input_step, hidden, cell, encoder_outputs):
    # input_step: (batch, 1, input_dim)
    # compute attention
    attn_weights = self.attention(hidden[-1], encoder_outputs)  # (batch, seq_len)
    attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch,1,enc_hidden)
    rnn_input = torch.cat((input_step, attn_applied), dim=2)
    output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
    pred = self.out(output.squeeze(1))
    return pred, hidden, cell, attn_weights

class Seq2SeqAttentionModel(nn.Module): def init(self, input_dim, enc_hidden_dim, dec_hidden_dim, output_window, device): super().init() self.encoder = Encoder(input_dim, enc_hidden_dim) self.decoder = Decoder(input_dim=1, enc_hidden_dim=enc_hidden_dim, dec_hidden_dim=dec_hidden_dim) self.device = device self.output_window = output_window

def forward(self, src):
    # src: (batch, seq_len, features)
    batch = src.size(0)
    encoder_outputs, (hidden, cell) = self.encoder(src)
    # decoder input: last observed y as first input
    decoder_input = src[:, -1:, 0].unsqueeze(2)  # (batch,1,1)
    outputs = []
    attn_weights_all = []
    for t in range(self.output_window):
        pred, hidden, cell, attn_weights = self.decoder(decoder_input, hidden, cell, encoder_outputs)
        outputs.append(pred.unsqueeze(1))
        attn_weights_all.append(attn_weights.unsqueeze(1))
        # teacher forcing not used here in forward; training loop may choose to use it
        decoder_input = pred.unsqueeze(1)
    outputs = torch.cat(outputs, dim=1)  # (batch, out_w, 1)
    attn = torch.cat(attn_weights_all, dim=1)  # (batch, out_w, seq_len)
    return outputs.squeeze(2), attn

---------------------------

Training & Evaluation

---------------------------

def train_model(model, dataloader, val_loader, epochs=10, lr=1e-3, device='cpu'): model = model.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=lr) criterion = nn.MSELoss() best_val = float('inf') history = {'train_loss': [], 'val_loss': []}

for epoch in range(epochs):
    model.train()
    train_losses = []
    for X, Y in dataloader:
        X = X.to(device)
        Y = Y.to(device)
        optimizer.zero_grad()
        preds, _ = model(X)
        loss = criterion(preds, Y)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())
    val_loss = evaluate_model(model, val_loader, device)
    avg_train = np.mean(train_losses)
    history['train_loss'].append(avg_train)
    history['val_loss'].append(val_loss)
    if val_loss < best_val:
        best_val = val_loss
        torch.save(model.state_dict(), 'best_attention_model.pt')
    print(f"Epoch {epoch+1}/{epochs} - train_loss: {avg_train:.4f} - val_loss: {val_loss:.4f}")
return history

def evaluate_model(model, dataloader, device='cpu'): model.eval() preds_all, targ_all = [], [] with torch.no_grad(): for X, Y in dataloader: X = X.to(device) Y = Y.to(device) preds, _ = model(X) preds_all.append(preds.cpu().numpy()) targ_all.append(Y.cpu().numpy()) preds_all = np.concatenate(preds_all, axis=0) targ_all = np.concatenate(targ_all, axis=0) rmse = math.sqrt(mean_squared_error(targ_all.flatten(), preds_all.flatten())) mase = mase_metric(targ_all, preds_all) return rmse

MASE implementation

def mase_metric(y_true, y_pred): # y_true, y_pred: (n_samples, out_h) n, h = y_true.shape # naive seasonal forecast using lag 1 on training set would be computed elsewhere; here approximate using first-diff mean mase_den = np.mean(np.abs(np.diff(y_true, axis=1))) if mase_den == 0: return np.nan mase_val = np.mean(np.abs(y_true - y_pred)) / mase_den return mase_val

---------------------------

Baselines

---------------------------

def train_linear_baseline(df_train, df_val, lags=[1,7,14,30]): tr = create_lag_features(df_train, lags) X_tr = tr[[f'y_lag_{l}' for l in lags]].values y_tr = tr['y'].values model = LinearRegression().fit(X_tr, y_tr) # validation evaluation: make naive rolling forecast by predicting next day using latest lags val = create_lag_features(pd.concat([df_train.tail(max(lags)), df_val], ignore_index=True), lags) X_val = val[[f'y_lag_{l}' for l in lags]].values y_val = val['y'].values preds = model.predict(X_val) rmse = math.sqrt(mean_squared_error(y_val, preds)) return {'model': model, 'rmse': rmse}

def train_lstm_baseline(df, input_window=60, output_window=14, device='cpu', epochs=5): # simpler seq2seq LSTM model without attention feature_cols = ['y','temp','holiday'] ds = TimeSeriesDataset(df, input_window=input_window, output_window=output_window, feature_cols=feature_cols) n = len(ds) split = int(n * 0.8) train_ds = torch.utils.data.Subset(ds, list(range(split))) val_ds = torch.utils.data.Subset(ds, list(range(split, n))) tr_dl = DataLoader(train_ds, batch_size=32, shuffle=True) val_dl = DataLoader(val_ds, batch_size=32)

class SimpleSeq2Seq(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_w):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.decoder = nn.LSTM(1, hidden_dim, batch_first=True)
        self.out = nn.Linear(hidden_dim, 1)
        self.output_w = output_w
    def forward(self, src):
        enc_out, (h,c) = self.encoder(src)
        dec_input = src[:, -1:, 0].unsqueeze(2)
        outs = []
        hidden, cell = h, c
        for _ in range(self.output_w):
            out, (hidden, cell) = self.decoder(dec_input, (hidden, cell))
            pred = self.out(out.squeeze(1))
            outs.append(pred.unsqueeze(1))
            dec_input = pred.unsqueeze(1)
        return torch.cat(outs, dim=1)

device = device
model = SimpleSeq2Seq(input_dim=3, hidden_dim=64, output_w=output_window).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()
for epoch in range(epochs):
    model.train()
    for X,Y in tr_dl:
        X=X.to(device); Y=Y.to(device)
        opt.zero_grad()
        preds = model(X)
        loss = loss_fn(preds, Y)
        loss.backward(); opt.step()
# evaluate RMSE on validation set
model.eval()
preds_all, y_all = [], []
with torch.no_grad():
    for X,Y in val_dl:
        X=X.to(device)
        p = model(X).cpu().numpy(); preds_all.append(p); y_all.append(Y.numpy())
preds_all = np.concatenate(preds_all, axis=0)
y_all = np.concatenate(y_all, axis=0)
rmse = math.sqrt(mean_squared_error(y_all.flatten(), preds_all.flatten()))
return {'model': model, 'rmse': rmse}

---------------------------

SARIMA baseline (if available)

---------------------------

def train_sarima(df_train, df_val, order=(1,1,1), seasonal_order=(0,1,1,7)): if not STATSMODELS_AVAILABLE: return {'error': 'statsmodels not available'} series = pd.concat([df_train['y'], df_val['y']]) # Fit on train portion model = SARIMAX(df_train['y'], order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False) res = model.fit(disp=False) forecast = res.predict(start=len(df_train), end=len(df_train)+len(df_val)-1) rmse = math.sqrt(mean_squared_error(df_val['y'].values, forecast.values)) return {'model': res, 'rmse': rmse}

---------------------------

Visualization & Report

---------------------------

def plot_series(df, savepath='series.png'): plt.figure(figsize=(12,4)) plt.plot(df['ds'], df['y'], label='y') plt.title('Generated Time Series') plt.xlabel('Date') plt.ylabel('Value') plt.tight_layout() plt.savefig(savepath) plt.close()

def plot_attention_example(attn, savepath='attention_example.png'): # attn: (out_h, seq_len) plt.figure(figsize=(8,4)) plt.imshow(attn, aspect='auto') plt.xlabel('Encoder time steps') plt.ylabel('Decoder output steps') plt.title('Attention weights (example)') plt.colorbar() plt.tight_layout() plt.savefig(savepath) plt.close()

def generate_report(metrics, filename='report.md'): now = datetime.utcnow().isoformat() lines = [f"# Time Series Forecasting Project Report\n", f"Generated: {now}\n", "## Summary\n", "This report summarizes model architectures, hyperparameters, and performance metrics (RMSE, MASE) for the attention model and baselines.\n", "## Metrics\n"] for k,v in metrics.items(): lines.append(f"- {k}: {v}\n") lines.append('\n## Notes\n- Attention model is seq2seq LSTM with Bahdanau attention.\n- Baselines: linear regression on lagged features, simpler LSTM, SARIMA (if available).\n') with open(filename,'w') as f: f.writelines('\n'.join(lines)) return filename

---------------------------

Orchestration: CLI

---------------------------

def main_stage_generate(args): df = generate_synthetic_timeseries() os.makedirs('data', exist_ok=True) df.to_csv('data/synthetic_ts.csv', index=False) plot_series(df, savepath='data/series.png') print('Generated dataset saved to data/synthetic_ts.csv and plot to data/series.png')

def main_stage_train_attention(args): df = pd.read_csv('data/synthetic_ts.csv') feature_cols = ['y','temp','holiday'] input_w = args.input_window out_w = args.output_window ds = TimeSeriesDataset(df, input_window=input_w, output_window=out_w, feature_cols=feature_cols) n = len(ds) split = int(n*0.8) train_ds = torch.utils.data.Subset(ds, list(range(split))) val_ds = torch.utils.data.Subset(ds, list(range(split, n))) tr_dl = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True) val_dl = DataLoader(val_ds, batch_size=args.batch_size)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = Seq2SeqAttentionModel(input_dim=3, enc_hidden_dim=64, dec_hidden_dim=64, output_window=out_w, device=device)
history = train_model(model, tr_dl, val_dl, epochs=args.epochs, lr=args.lr, device=device)
print('Training complete. Best model saved to best_attention_model.pt')

# save one attention example
model.load_state_dict(torch.load('best_attention_model.pt', map_location=device))
model.eval()
Xsample, Ysample = ds.X[:1], ds.Y[:1]
with torch.no_grad():
    Xt = torch.from_numpy(Xsample).to(device)
    preds, attn = model(Xt)
attn_np = attn[0].cpu().numpy()  # (out_w, seq_len)
plot_attention_example(attn_np, savepath='data/attention_example.png')
print('Saved attention example to data/attention_example.png')

def main_stage_train_baselines(args): df = pd.read_csv('data/synthetic_ts.csv') # split n = len(df) train = df.iloc[:int(n0.8)].reset_index(drop=True) val = df.iloc[int(n0.8):].reset_index(drop=True) lr_res = train_linear_baseline(train, val) print('Linear baseline RMSE:', lr_res['rmse']) lstm_res = train_lstm_baseline(df, input_window=args.input_window, output_window=args.output_window, epochs=args.epochs) print('LSTM baseline RMSE:', lstm_res['rmse']) if STATSMODELS_AVAILABLE: sarima_res = train_sarima(train, val) print('SARIMA RMSE:', sarima_res['rmse']) metrics = {'linear_rmse': lr_res['rmse'], 'lstm_rmse': lstm_res['rmse']} if STATSMODELS_AVAILABLE: metrics['sarima_rmse'] = sarima_res['rmse'] with open('metrics_summary.json','w') as f: json.dump(metrics, f, indent=2) print('Baselines trained and metrics saved to metrics_summary.json')

def main_stage_eval_all(args): # load metrics, produce report if os.path.exists('metrics_summary.json'): with open('metrics_summary.json') as f: metrics = json.load(f) else: metrics = {} # if attention model present, evaluate quickly on validation set if os.path.exists('best_attention_model.pt') and os.path.exists('data/synthetic_ts.csv'): df = pd.read_csv('data/synthetic_ts.csv') feature_cols = ['y','temp','holiday'] ds = TimeSeriesDataset(df, input_window=args.input_window, output_window=args.output_window, feature_cols=feature_cols) n = len(ds) split = int(n*0.8) val_ds = torch.utils.data.Subset(ds, list(range(split, n))) val_dl = DataLoader(val_ds, batch_size=32) device = 'cuda' if torch.cuda.is_available() else 'cpu' model = Seq2SeqAttentionModel(input_dim=3, enc_hidden_dim=64, dec_hidden_dim=64, output_window=args.output_window, device=device) model.load_state_dict(torch.load('best_attention_model.pt', map_location=device)) rmse = evaluate_model(model, val_dl, device=device) metrics['attention_rmse'] = rmse report_file = generate_report(metrics) print('Report generated:', report_file)

if name == 'main': parser = argparse.ArgumentParser() parser.add_argument('--stage', type=str, default='generate', choices=['generate','train_attention','train_baselines','eval_all']) parser.add_argument('--input_window', type=int, default=60) parser.add_argument('--output_window', type=int, default=14) parser.add_argument('--epochs', type=int, default=5) parser.add_argument('--batch_size', type=int, default=32) parser.add_argument('--lr', type=float, default=1e-3) args = parser.parse_args()

if args.stage == 'generate':
    main_stage_generate(args)
elif args.stage == 'train_attention':
    main_stage_train_attention(args)
elif args.stage == 'train_baselines':
    main_stage_train_baselines(args)
elif args.stage == 'eval_all':
    main_stage_eval_all(args)